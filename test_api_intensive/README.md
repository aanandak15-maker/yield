# API Intensive Testing Framework

Comprehensive testing framework for the Crop Yield Prediction API, providing functional, performance, security, and integration testing capabilities.

## Directory Structure

```
test_api_intensive/
├── __init__.py                 # Package initialization
├── conftest.py                 # Shared pytest fixtures
├── pytest.ini                  # Pytest configuration
├── requirements.txt            # Test dependencies
├── setup.py                    # Package setup
├── README.md                   # This file
├── run_tests.py               # Main test runner (to be created)
├── config/                    # Configuration files
│   ├── __init__.py
│   ├── test_config.json       # Test configuration (to be created)
│   └── test_data.json         # Test data fixtures (to be created)
├── suites/                    # Test suites
│   ├── __init__.py
│   ├── test_functional.py     # Functional tests (to be created)
│   ├── test_variety_selection.py
│   ├── test_validation.py
│   ├── test_performance.py
│   ├── test_load.py
│   ├── test_error_handling.py
│   ├── test_security.py
│   ├── test_integration.py
│   ├── test_backward_compat.py
│   └── test_end_to_end.py
├── utils/                     # Utility modules
│   ├── __init__.py
│   ├── api_client.py          # API client wrapper (to be created)
│   ├── test_data_generator.py # Test data generation (to be created)
│   ├── assertions.py          # Custom assertions (to be created)
│   ├── performance_metrics.py # Performance measurement (to be created)
│   └── mock_services.py       # Mock external services (to be created)
└── reports/                   # Generated test reports
    └── .gitkeep
```

## Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager
- Virtual environment (recommended)

### Setup

1. Create and activate a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

2. Install dependencies:

```bash
cd test_api_intensive
pip install -r requirements.txt
```

Or install the package in development mode:

```bash
pip install -e .
```

## Configuration

### Test Configuration

Edit `config/test_config.json` to configure:
- API base URL
- Timeout settings
- Performance thresholds
- Test data locations
- Concurrent user counts

### Environment Variables

You can override configuration using environment variables:
- `API_BASE_URL`: API base URL (default: http://localhost:8000)
- `API_TIMEOUT`: Request timeout in seconds (default: 30)

## Running Tests

### Run All Tests

```bash
pytest
```

### Run Specific Test Suite

```bash
# Run functional tests only
pytest suites/test_functional.py

# Run performance tests only
pytest suites/test_performance.py
```

### Run Tests by Marker

```bash
# Run only fast tests
pytest -m fast

# Run only critical tests
pytest -m critical

# Run functional and validation tests
pytest -m "functional or validation"

# Exclude slow tests
pytest -m "not slow"
```

### Parallel Execution

```bash
# Run tests in parallel (auto-detect CPU count)
pytest -n auto

# Run tests with 4 workers
pytest -n 4
```

### Generate HTML Report

```bash
# HTML report is generated by default in reports/test_report.html
pytest

# Custom report location
pytest --html=reports/custom_report.html
```

### Verbose Output

```bash
# Verbose output with test details
pytest -v

# Very verbose with print statements
pytest -vv -s
```

### Run Specific Test

```bash
# Run a specific test function
pytest suites/test_functional.py::TestEndpoints::test_predict_yield

# Run all tests in a class
pytest suites/test_functional.py::TestEndpoints
```

## Test Markers

Tests are organized using pytest markers:

- `@pytest.mark.functional` - Functional tests for API endpoints
- `@pytest.mark.variety_selection` - Variety selection tests
- `@pytest.mark.validation` - Input validation tests
- `@pytest.mark.performance` - Performance tests
- `@pytest.mark.load` - Load and stress tests
- `@pytest.mark.error_handling` - Error handling tests
- `@pytest.mark.security` - Security tests
- `@pytest.mark.integration` - Integration tests
- `@pytest.mark.backward_compat` - Backward compatibility tests
- `@pytest.mark.end_to_end` - End-to-end tests
- `@pytest.mark.monitoring` - Monitoring and logging tests
- `@pytest.mark.slow` - Slow tests (> 5 seconds)
- `@pytest.mark.fast` - Fast tests (< 1 second)
- `@pytest.mark.critical` - Critical tests that must pass

## Test Reports

Test reports are generated in the `reports/` directory:

- `test_report.html` - HTML test report with detailed results
- `test_execution.log` - Detailed execution log
- `coverage/` - Code coverage reports (if enabled)
- Performance metrics exports (JSON/CSV)

## Writing Tests

### Basic Test Structure

```python
import pytest
from utils.api_client import CropYieldAPIClient
from utils.assertions import assert_valid_prediction_response

@pytest.mark.functional
class TestYieldPrediction:
    """Test suite for yield prediction endpoint"""
    
    @pytest.fixture
    def api_client(self, api_base_url):
        """Create API client fixture"""
        return CropYieldAPIClient(api_base_url)
    
    def test_predict_with_variety(self, api_client):
        """Test prediction with specified variety"""
        response = api_client.predict_yield(
            crop_type="Rice",
            latitude=23.2599,
            longitude=77.4126,
            sowing_date="2024-06-15",
            variety_name="IR-64"
        )
        
        assert_valid_prediction_response(response)
        assert response.status_code == 200
```

### Using Test Data Generator

```python
from utils.test_data_generator import TestDataGenerator

def test_with_generated_data(api_client):
    """Test with generated test data"""
    generator = TestDataGenerator()
    request_data = generator.generate_valid_request(crop_type="Rice")
    
    response = api_client.predict_yield(**request_data)
    assert response.is_success()
```

## Performance Testing

Performance tests measure response times and throughput:

```bash
# Run performance tests
pytest -m performance

# Run load tests (may take several minutes)
pytest -m load
```

### Load and Stress Testing

The framework includes comprehensive load and stress testing:

```bash
# Run all load tests
pytest suites/test_load.py -v -s

# Run specific load test
pytest suites/test_load.py::TestLoadAndStress::test_gradual_ramp_up -v -s

# Use Locust for interactive load testing
locust -f suites/test_load.py --host=http://localhost:8000
```

**Load Test Types**:
- **Ramp-up**: Gradual increase from 1 to 100 users
- **Sustained**: 100 users for 5 minutes
- **Spike**: Sudden increase from 10 to 150 users
- **Stress**: 200 users beyond capacity
- **Recovery**: System recovery after stress

**See Also**:
- `RUNNING_LOAD_TESTS.md` - Detailed load testing guide
- `LOAD_TESTS_QUICK_REFERENCE.md` - Quick reference
- `TASK_8_LOAD_STRESS_TESTS_SUMMARY.md` - Implementation details

## Troubleshooting

### Tests Fail to Connect to API

- Ensure the API is running at the configured base URL
- Check `API_BASE_URL` environment variable or `test_config.json`
- Verify network connectivity

### Timeout Errors

- Increase timeout in `test_config.json` or `pytest.ini`
- Check if API is responding slowly
- Use `pytest --timeout=60` to override timeout

### Import Errors

- Ensure all dependencies are installed: `pip install -r requirements.txt`
- Verify you're in the correct directory
- Check Python version (3.8+ required)

### Parallel Execution Issues

- Some tests may not be thread-safe
- Try running without parallel execution: `pytest` (without `-n` flag)
- Check for shared state between tests

## Contributing

### Adding New Tests

1. Create test file in appropriate suite directory
2. Use appropriate markers for categorization
3. Follow existing test patterns
4. Add docstrings to test functions
5. Update this README if adding new test categories

### Code Style

- Follow PEP 8 guidelines
- Use type hints where appropriate
- Add docstrings to functions and classes
- Keep tests focused and independent

## CI/CD Integration

The test framework can be integrated into CI/CD pipelines:

```yaml
# Example GitHub Actions workflow
- name: Run API Tests
  run: |
    cd test_api_intensive
    pip install -r requirements.txt
    pytest -n auto --html=reports/test_report.html
```

## Support

For issues or questions:
1. Check the troubleshooting section
2. Review test execution logs in `reports/test_execution.log`
3. Consult the design document at `.kiro/specs/api-intensive-testing/design.md`

## License

Internal use only - Crop Yield Prediction API Testing Framework
